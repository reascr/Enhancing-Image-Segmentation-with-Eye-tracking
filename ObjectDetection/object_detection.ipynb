{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pascal to coco converter\n",
    "pascal_to_coco = {\n",
    "    \"aeroplane\": \"airplane\",\n",
    "    \"bicycle\": \"bicycle\",\n",
    "    \"bird\": \"bird\",\n",
    "    \"boat\": \"boat\",\n",
    "    \"bottle\": \"bottle\",\n",
    "    \"bus\": \"bus\",\n",
    "    \"car\": \"car\",\n",
    "    \"cat\": \"cat\",\n",
    "    \"chair\": \"chair\",\n",
    "    \"cow\": \"cow\",\n",
    "    \"diningtable\": \"dining table\",\n",
    "    \"dog\": \"dog\",\n",
    "    \"horse\": \"horse\",\n",
    "    \"motorbike\": \"motorcycle\",\n",
    "    \"person\": \"person\",\n",
    "    \"pottedplant\": \"potted plant\",\n",
    "    \"sheep\": \"sheep\",\n",
    "    \"sofa\": \"couch\",\n",
    "    \"train\": \"train\",\n",
    "    \"tvmonitor\": \"monitor\"\n",
    "}\n",
    "\n",
    "# Setting up categories\n",
    "coco_names = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \n",
    "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', \n",
    "    'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', \n",
    "    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', \n",
    "    'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag', 'tie', \n",
    "    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', \n",
    "    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', \n",
    "    'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork', \n",
    "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', \n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', \n",
    "    'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', \n",
    "    'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', \n",
    "    'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', \n",
    "    'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\magnu\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\magnu\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eye tracking data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing eye tracking data\n",
    "def process_eye_tracking_data(CSV_PATH):\n",
    "    # Prepare variables for analysis\n",
    "    eye_tracking_data = pd.read_csv(CSV_PATH)\n",
    "    grouped_data = eye_tracking_data.groupby([\"image_file\", \"class\"])[\"response\"]\n",
    "    image_data = {}\n",
    "\n",
    "    # Go through the eye tracking data and compute\n",
    "    for idx, ((img, target_class), res) in enumerate(grouped_data):\n",
    "        # Compute majority vote\n",
    "        res_list = list(res)\n",
    "        majority_vote = round(sum(res_list) / len(res_list))\n",
    "        agreement_ratio = sum(res_list) / len(res_list)\n",
    "\n",
    "        # Populate image data\n",
    "        image_data[img] = {\n",
    "            \"responses\": res_list,\n",
    "            \"mv\": int(majority_vote),\n",
    "            \"agreement_ratio\": agreement_ratio,\n",
    "            \"class\": target_class\n",
    "        }   \n",
    "\n",
    "    # Return statement\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for prediction\n",
    "def predict(input_tensor, model, device, detection_threshold):\n",
    "    outputs = model(input_tensor)\n",
    "    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    \n",
    "    boxes, classes, labels, indices = [], [], [], []\n",
    "    for index in range(len(pred_scores)):\n",
    "        if pred_scores[index] >= detection_threshold:\n",
    "            boxes.append(pred_bboxes[index].astype(np.int32))\n",
    "            classes.append(pred_classes[index])\n",
    "            labels.append(pred_labels[index])\n",
    "            indices.append(index)\n",
    "    boxes = np.int32(boxes)\n",
    "\n",
    "    return boxes, classes, labels, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for retrieving annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_annotation(image_name, TAR_PATH, ANNOTATION_PATH=\"VOCdevkit/VOC2012/Annotations\"):\n",
    "    target_file = f\"{ANNOTATION_PATH}/{image_name}.xml\"\n",
    "    # print(target_file) # PRINT STATEMENT \n",
    "\n",
    "    # Find the file\n",
    "    with tarfile.open(TAR_PATH, \"r\") as tar:\n",
    "        try:\n",
    "            member = tar.getmember(target_file)\n",
    "            if member.isfile():\n",
    "                with tar.extractfile(member) as file:\n",
    "                    content = file.read().decode(\"utf-8\")\n",
    "                    return content\n",
    "        except KeyError:\n",
    "            print(f\"Error: {target_file} not found in the tar archive.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for extracting bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_bboxes_from_annotation(xml_content: str, class_name: str): # Changed from coco class\n",
    "    try:\n",
    "        root = ET.fromstring(xml_content)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return []\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    # Iterate over all object elements in the XML\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text.strip().lower()\n",
    "\n",
    "        # Check if the object's name matches the target_class\n",
    "        if name == class_name.lower(): # Changed from coco class\n",
    "            bndbox = obj.find('bndbox')\n",
    "            if bndbox is not None:\n",
    "                try:\n",
    "                    bbox = {\n",
    "                        'xmin': int(bndbox.find('xmin').text),\n",
    "                        'ymin': int(bndbox.find('ymin').text),\n",
    "                        'xmax': int(bndbox.find('xmax').text),\n",
    "                        'ymax': int(bndbox.find('ymax').text)\n",
    "                    }\n",
    "                    bboxes.append(bbox)\n",
    "                except (AttributeError, ValueError) as e:\n",
    "                    print(f\"Error extracting bounding box coordinates: {e}\")\n",
    "                    continue\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for calculating the intersection over union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxA, boxB):\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA['xmin'], boxB['xmin'])\n",
    "    yA = max(boxA['ymin'], boxB['ymin'])\n",
    "    xB = min(boxA['xmax'], boxB['xmax'])\n",
    "    yB = min(boxA['ymax'], boxB['ymax'])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interWidth = max(0, xB - xA + 1)\n",
    "    interHeight = max(0, yB - yA + 1)\n",
    "    interArea = interWidth * interHeight\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA['xmax'] - boxA['xmin'] + 1) * (boxA['ymax'] - boxA['ymin'] + 1)\n",
    "    boxBArea = (boxB['xmax'] - boxB['xmin'] + 1) * (boxB['ymax'] - boxB['ymin'] + 1)\n",
    "\n",
    "    # Compute the Intersection over Union\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ZIP_PATH_POET: str, TAR_PATH_ANNOTATION: str, CSV_PATH: str):\n",
    "    # Retrieve the eye tracking data\n",
    "    eye_tracking_data = process_eye_tracking_data(CSV_PATH=CSV_PATH)\n",
    "    print(f\"Length of eye tracking data: {len(eye_tracking_data)}\")\n",
    "\n",
    "    # Variable for the final data\n",
    "    combined_data = []\n",
    "\n",
    "    # Go through the zipfile\n",
    "    with zipfile.ZipFile(ZIP_PATH_POET, \"r\") as zr:\n",
    "        print(f\"# Number of files in zip: {len(zr.namelist())}\")\n",
    "\n",
    "        # Counter to stop the process\n",
    "        # counter = 0\n",
    "\n",
    "        # Go through images processed in the eye tracking data\n",
    "        for idx, (image_name, data) in enumerate(eye_tracking_data.items()):\n",
    "            # Deciding if it should stop\n",
    "            # if counter >= 10:\n",
    "            #     break\n",
    "            \n",
    "            # Status print\n",
    "            if idx % 200 == 0:\n",
    "                print(f\"Processing image {idx + 1} of {len(eye_tracking_data)}\")\n",
    "            \n",
    "            # Retrieving class and creating path\n",
    "            class_name = data[\"class\"].lower()\n",
    "            coco_class = pascal_to_coco[class_name]\n",
    "            FULL_PATH = f\"POETdataset/PascalImages/{class_name}_{image_name}.jpg\"\n",
    "            # print(f\"Pascal class: {class_name}, coco class: {coco_class}\") # PRINT STATEMENT \n",
    "            # print(f\"Full constructed path: {FULL_PATH}\") # PRINT STATEMENT\n",
    "\n",
    "            # Check if the full path is in zip path\n",
    "            if FULL_PATH not in zr.namelist():\n",
    "                print(f\"Image {FULL_PATH} not found in zip\")\n",
    "                continue\n",
    "\n",
    "            # Load the image\n",
    "            with zr.open(FULL_PATH) as file:\n",
    "                image = Image.open(file)\n",
    "\n",
    "                # Transform the image for processing\n",
    "                transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "                input_tensor = transform(image).to(device).unsqueeze(0)\n",
    "                # print(f\"Input Tensor: {input_tensor}\")\n",
    "\n",
    "                # Making the prediction\n",
    "                boxes, classes, labels, indices = predict(input_tensor, model, device, 0.0)\n",
    "\n",
    "                # Getting coco class index and sorting the boxes\n",
    "                class_index = coco_names.index(coco_class)\n",
    "                current_boxes = [box for box, label in zip(boxes, labels) if label == class_index]\n",
    "                # print(f\"Class index: {class_index}, {current_boxes}\") # PRINT STATEMENT\n",
    "\n",
    "                # print(f\"Boxes: {len(boxes)}, Classes: {len(classes)}, Labels: {len(labels)}\") # PRINT STATEMENT\n",
    "\n",
    "                # Finding the annotations folder from the tar file\n",
    "                annotations = find_annotation(\n",
    "                    image_name=image_name,\n",
    "                    TAR_PATH=TAR_PATH_ANNOTATION,\n",
    "                )\n",
    "\n",
    "                # Continue if there is no annotations\n",
    "                if annotations:\n",
    "                    gtbb = extract_bboxes_from_annotation(annotations, class_name)\n",
    "                    # print(gtbb) # PRINT STATEMENT\n",
    "                else:\n",
    "                    combined_data.append({\n",
    "                        \"image_name\": f\"{class_name}_{image_name}\",\n",
    "                        \"responses\": data['responses'],\n",
    "                        \"majority_vote\": data['mv'],\n",
    "                        \"agreement_ratio\": data['agreement_ratio'],\n",
    "                        \"model_detection\": None,\n",
    "                        \"agreement\": None,  # Since no ground truth, no agreement\n",
    "                        \"class\": data['class'],\n",
    "                        \"zip_file_path\": \"POETdataset.zip/\" + FULL_PATH\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # Convert the predicted boxes to dictionary\n",
    "                predicted_boxes = []\n",
    "                for box in current_boxes:\n",
    "                    predicted_boxes.append({\n",
    "                        \"xmin\": int(box[0]),\n",
    "                        \"ymin\": int(box[1]),\n",
    "                        \"xmax\": int(box[2]),\n",
    "                        \"ymax\": int(box[3])\n",
    "                    })\n",
    "\n",
    "                # iou flag\n",
    "                detection_flag = 0\n",
    "                IOU_THRESHOLD = 0.5\n",
    "\n",
    "                # Go through the bounding boxes\n",
    "                for gtb in gtbb:\n",
    "                    for pb in predicted_boxes:\n",
    "                        iou = calculate_iou(gtb, pb)\n",
    "                        if iou >= IOU_THRESHOLD:\n",
    "                            # print(\"Success\") # PRINT STATEMENT \n",
    "                            detection_flag = 1\n",
    "                            break\n",
    "                    if detection_flag == 1:\n",
    "                        break\n",
    "                \n",
    "                # Printing the flag for check\n",
    "                # print(f\"Final iou flag: {detection_flag}\") # PRINT STATEMENT\n",
    "\n",
    "                # Calculating agreement\n",
    "                agreement = detection_flag == data['mv']\n",
    "                # print(f\"Agreement: {agreement}\") # PRINT STATEMENT\n",
    "\n",
    "                # Populating the data structure\n",
    "                combined_data.append({\n",
    "                    \"image_name\": f\"{class_name}_{image_name}\",\n",
    "                    \"responses\": data['responses'],\n",
    "                    \"majority_vote\": data['mv'],\n",
    "                    \"agreement_ratio\": data['agreement_ratio'],\n",
    "                    \"model_detection\": detection_flag,\n",
    "                    \"agreement\": agreement,\n",
    "                    \"class\": data['class'],\n",
    "                    \"zip_file_path\": \"POETdataset.zip/\" + FULL_PATH\n",
    "                })\n",
    "\n",
    "            # Increasing the counter\n",
    "            # counter += 1\n",
    "    \n",
    "    # Return statement\n",
    "    return eye_tracking_data, combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip path for poet dataset: POETdataset.zip\n",
      "Zip path for annotations: VOCtrainval_11-May-2012.tar\n",
      "Csv path for eye tracking data: ../eye_tracking_data.csv\n",
      "Length of eye tracking data: 6131\n",
      "# Number of files in zip: 6311\n",
      "Processing image 1 of 6131\n",
      "Processing image 201 of 6131\n",
      "Processing image 401 of 6131\n",
      "Processing image 601 of 6131\n",
      "Processing image 801 of 6131\n",
      "Processing image 1001 of 6131\n",
      "Processing image 1201 of 6131\n",
      "Processing image 1401 of 6131\n",
      "Processing image 1601 of 6131\n",
      "Processing image 1801 of 6131\n",
      "Processing image 2001 of 6131\n",
      "Processing image 2201 of 6131\n",
      "Processing image 2401 of 6131\n",
      "Processing image 2601 of 6131\n",
      "Processing image 2801 of 6131\n",
      "Processing image 3001 of 6131\n",
      "Processing image 3201 of 6131\n",
      "Processing image 3401 of 6131\n",
      "Processing image 3601 of 6131\n",
      "Processing image 3801 of 6131\n",
      "Processing image 4001 of 6131\n",
      "Processing image 4201 of 6131\n",
      "Processing image 4401 of 6131\n",
      "Processing image 4601 of 6131\n",
      "Processing image 4801 of 6131\n",
      "Processing image 5001 of 6131\n",
      "Processing image 5201 of 6131\n",
      "Processing image 5401 of 6131\n",
      "Processing image 5601 of 6131\n",
      "Processing image 5801 of 6131\n",
      "Processing image 6001 of 6131\n"
     ]
    }
   ],
   "source": [
    "ZIP_PATH_POET = \"POETdataset.zip\"\n",
    "TAR_PATH_ANNOTATION = \"VOCtrainval_11-May-2012.tar\"\n",
    "CSV_PATH = \"../eye_tracking_data.csv\"\n",
    "\n",
    "# Printin the paths for confirmation\n",
    "print(f\"Zip path for poet dataset: {ZIP_PATH_POET}\\nZip path for annotations: {TAR_PATH_ANNOTATION}\\nCsv path for eye tracking data: {CSV_PATH}\")\n",
    "\n",
    "# Calling the main function\n",
    "eye_tracking_data, combined_data = main(\n",
    "    ZIP_PATH_POET=ZIP_PATH_POET,\n",
    "    TAR_PATH_ANNOTATION=TAR_PATH_ANNOTATION,\n",
    "    CSV_PATH=CSV_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved successfully\n"
     ]
    }
   ],
   "source": [
    "combined_data_df = pd.DataFrame(combined_data)\n",
    "combined_data_df.head()\n",
    "combined_data_df.to_csv('combined_data_07_with_gtbb_correct.csv', index=False)\n",
    "print(\"Dataframe saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.89\n",
      "Eye-Tracking Accuracy: 0.88\n",
      "Model F1 Score: 0.94\n",
      "Eye-Tracking F1 Score: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_accuracy': 0.885336812917958,\n",
       " 'eye_tracking_accuracy': 0.8848474963301256,\n",
       " 'model_f1': 0.9391815901029501,\n",
       " 'eye_tracking_f1': 0.9389061959155417}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_accuracy_and_f1(combined_data):\n",
    "    # Initialize lists to store true labels and predictions for both the model and eye-tracking system\n",
    "    model_true_labels = []\n",
    "    model_predictions = []\n",
    "    eye_tracking_true_labels = []\n",
    "    eye_tracking_predictions = []\n",
    "\n",
    "    for data in combined_data:\n",
    "        # For the model:\n",
    "        model_true_labels.append(1)  # True label is always 1 (class is always present)\n",
    "        model_predictions.append(data[\"model_detection\"])  # Model's prediction (1 or 0)\n",
    "\n",
    "        # For the eye-tracking system:\n",
    "        eye_tracking_true_labels.append(1)  # True label is always 1 (class is always present)\n",
    "        eye_tracking_predictions.append(data[\"majority_vote\"])  # Eye-tracking majority vote (1 or 0)\n",
    "\n",
    "    # Calculate accuracy for the model and eye-tracking system\n",
    "    model_accuracy = sum(1 for pred in model_predictions if pred == 1) / len(model_predictions)\n",
    "    eye_tracking_accuracy = sum(1 for pred in eye_tracking_predictions if pred == 1) / len(eye_tracking_predictions)\n",
    "\n",
    "    # Calculate F1 score for the model and eye-tracking system\n",
    "    model_f1 = f1_score(model_true_labels, model_predictions)\n",
    "    eye_tracking_f1 = f1_score(eye_tracking_true_labels, eye_tracking_predictions)\n",
    "\n",
    "    # Combine results into a dictionary\n",
    "    result = {\n",
    "        \"model_accuracy\": model_accuracy,\n",
    "        \"eye_tracking_accuracy\": eye_tracking_accuracy,\n",
    "        \"model_f1\": model_f1,\n",
    "        \"eye_tracking_f1\": eye_tracking_f1\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "results = calculate_accuracy_and_f1(combined_data)\n",
    "\n",
    "# Print combined results\n",
    "print(f\"Model Accuracy: {results['model_accuracy']:.2f}\")\n",
    "print(f\"Eye-Tracking Accuracy: {results['eye_tracking_accuracy']:.2f}\")\n",
    "print(f\"Model F1 Score: {results['model_f1']:.2f}\") \n",
    "print(f\"Eye-Tracking F1 Score: {results['eye_tracking_f1']:.2f}\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
